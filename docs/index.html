<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Prime Directive</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <header>
        <nav class="navbar">
            <div class="container">
                <h2 class="logo">Prime Directive</h2>
                <ul class="nav-links">
                    <li><a href="#abstract">Abstract</a></li>
                    <li><a href="#description">Description</a></li>
                    <li><a href="#team">Team</a></li>
                </ul>
            </div>
        </nav>
    </header>

    <main>
        <section class="hero">
            <div class="container">
                <h1>P.O.T.A.T.O.</h1>
                <p class="subtitle">A ROS2-based autonomous system with computer vision and LiDAR mapping</p>
            </div>
        </section>

        <section id="abstract" class="abstract">
            <div class="container">
                <h2>Project Abstract</h2>
                <div class="abstract-content">
                    <p><strong>P.O.T.A.T.O.</strong> Parcel Optically Terrain Aware Traversing Operator</p>
                    <p>An autonomous rover designed to meet the payload challenge at the 2026 Intercollegiate Rocket Engineering Competition (IREC). The rover will transition through 3 stages: flight, exploration, and recovery. During flight, video will be transmitted to the ground station over a 5.8GHz radio link. Upon touchdown, the system detects landing, initializes sensors, and builds a virtual map of the surrounding terrain using lidar, GPS, and SLAM. Through the utilization of computer vision and obstacle avoidance technologies, the rover will begin navigation back to the launch site, sounding a buzzer if it runs out of battery before reaching its destination.</p>
                </div>
            </div>
        </section>

        <section id="description" class="description">
            <div class="container">
                <h2>Project Description</h2>
                
                <div class="desc-subsection">
                    <h3>What We Built</h3>
                    <p>The foundation of a rover capable of autonomous navigation and terrain analysis using advanced sensors and computer vision. Currently, the rover is capable of transmitting video to a remote device and collecting LiDAR data to be used in the creation of a map of the scanned environment.</p>
                </div>

                <div class="desc-subsection">
                    <h3>How It Works</h3>
                    <p><strong>LiDAR Mapping Node:</strong> Takes raw data input from a Ylidar GS2 and turns it into a standardized format, publishes it to the /scan ROS2 topic, to then be visualized in RVIZ software.</p>
                    <p><strong>Pi Camera 3 Streaming Node:</strong> Utilizing a Pi Cam 3 module, takes images on an interval publishing them to a /cam ROS2 topic. This is then matched with a consumer node that can be run on a separate device wirelessly to stream video with per-frame controls.</p>
                    <p><strong>OpenCV Development:</strong> Currently being trained on a variety of image processing techniques for object detection and terrain analysis.</p>
                    <div class="desc-subsection">
                        <h3>How AI Uses Mathematics to Interpret Images</h3>
                        <p>Artificial intelligence interprets images by converting them into arrays of numbers, where each value represents a pixelâ€™s color or intensity. Using mathematical operations such as convolution, AI models like neural networks detect patterns, edges, and shapes within these arrays. Techniques like matrix multiplication and statistical analysis allow the system to recognize objects, classify scenes, and extract features. By training on large datasets, the AI learns to associate mathematical patterns with real-world objects, enabling accurate image interpretation and decision-making in complex environments.</p>
                    </div>
                </div>

                <div class="desc-subsection">
                    <h3>Screenshots & Media</h3>
                    <div class="screenshots">
                        <figure>
                            <img src="./Screenshot1.png" alt="System overview">
                            <figcaption>The POTATO being ejected</figcaption>
                        </figure>
                        <figure>
                            <img src="./Screenshot2.png" alt="LiDAR mapping output">
                            <figcaption>Potato CAD Model</figcaption>
                        </figure>
                        <figure>
                            <img src="./Screenshot3.png" alt="Camera feed">
                            <figcaption>Boise Rocketry Club</figcaption>
                        </figure>
                    </div>
                </div>
            </div>
        </section>

        <section id="team" class="team">
            <div class="container">
                <h2>Team Members</h2>
                <div class="team-grid">
                    <div class="team-member">
                        <h3>Jack Lawford</h3>
                        <p>LiDAR Scanning and Mapping</p>
                    </div>
                    <div class="team-member">
                        <h3>Ricardo Rodriguez</h3>
                        <p>Real-Time Video Streaming</p>
                    </div>
                    <div class="team-member">
                        <h3>Emi Dretcanu</h3>
                        <p>Computer Vision Development</p>
                    </div>
                </div>
            </div>
        </section>
    </main>

    <footer>
        <div class="container">
            <p>&copy; 2025 Prime Directive. View on <a href="https://github.com/Keys-481/fa25-prime-directive/" target="_blank">GitHub</a></p>
        </div>
    </footer>
</body>
</html>
